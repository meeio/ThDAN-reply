\section{How does the domain discriminator $G_d$ work? $G_d$ should be explained when it first appears.}

\subsection*{\underline{\textbf{Response:}}}

Thanks for your comments and suggestions.
$G_d$ is a binary domain identifier with all the source samples labelled as $0$ and all the target samples labelled as $1$.

$G_d$ is an essential component in domain adversarial training.
In the domain adversarial training, a domain discriminator is trained to separate the feature representation of the source domain from the target domain, 
and a feature generator is trained to deceive the domain discriminator.
Formally, the training procedure can be written as,
\begin{equation}
    \label{eq: training DANN}
    \begin{split}
        \min_{G_f^t} \max_{G_d} \mathscr{L}(G_d,G^{s}_{f},G_f^t) &=\mathbb{E}_{x\sim p_t(x)} \left[ \log \left(G_d\left(G_f^t\left(x\right)\right)\right) \right]\\
        &+\mathbb{E}_{x\sim p_s(x)}\left[ \log \left(1-G_d\left(G_f^s\left(x\right)\right)\right) \right].
    \end{split}
\end{equation}
Here $p_s (x)$ and $p_t (x)$ are probability distributions of source and target data respectively.
$G_d$ aims to label source samples as $0$ and target samples as $1$.
$G_f^s$ and $G_f^t$ are the feature extractors for source and target samples, which share weights as in \cite{OpensetDA-bp}.

To clarify how the domain discriminator $G_d$ and domain adversarial training works, we revised the manuscript by adding \textit{Section 3.2. Preliminary: Domain Adversarial Training} for better understanding.


\section{In \textcolor{blue}{Eq.(2)}, $p_s$ and $p_t$ are probability distributions, so what do $p_s (z_t)$ and $p_t (z_t)$ mean? And why is the optimal $G_d$ this? }
\subsection*{\underline{\textbf{Response:}}}

Sorry for the misleading symbol notations. 
In \textcolor{blue}{Eq.(2)} of the original manuscript, the subscript of $z_t$ should be removed and $z$ is the deep feature representation of a sample, \textit{i.e.,} $z=G_f(x).$
As stated in \textit{Section 3.1 open set domain adaptation} of the revised manuscript, $p_s (z)$ and $p_t (z)$ are probability distributions of feature representations in source and target domains respectively, as defined in \cite{PartialDA-iw}.
In the revised manuscript, \textcolor{blue}{Eq.(2)} is rewritten as:
\begin{equation}
    \label{eq: revised optimal}
    \begin{split}
        G_d^*(z) &= \frac{p_t(z)}{p_s(z)+p_t(z)}. \\
    \end{split}
\end{equation}

Similar to \cite{goodfellow2014generative}, the proof for the property that Eq.(\ref{eq: revised optimal}) gives the optimal $G_d$ is as follows,
\begin{proof}
    For any $G_f^s$ and $G_f^t$, we train $G_d$ to maximize Eq.(\ref{eq: training DANN}):
    \begin{equation}
        \label{eq: proof optimal discriminator}
        \begin{split}
            \max_{G_d} \mathscr{L}(G_d,G^{s}_{f},G_f^t)  = &\int_x p_t(x)\log \left(G_d\left(G_f^t\left(x\right)\right)\right)
              + p_s(x) \log\left(1-G_d\left(G_f^s\left(x\right)\right)\right) \, dx.
            \\ = &\int_z p_t(z)\log \left(G_d\left(z\right)\right)
              + p_s(z) \log\left(1-G_d\left(z\right)\right) \, dz.
        \end{split}
    \end{equation}
    We take the partial differential of the objective Eq.(\ref{eq: proof optimal discriminator}) with respect to $G_d$, and apply the Leibnizs rule to exchange the order of differentiation and integration to achieve optimal $G_d$ in $[0, 1]$ at Eq.(\ref{eq: revised optimal}).
\end{proof}

The proof has been added in \textit{Section 3.2. Preliminary: Domain Adversarial Training} of the revised manuscript.

\section{In \textcolor{blue}{Eq.(4)} and \textcolor{blue}{Eq.(5)}, authors use the entropy of probabilities of known classes to measure the probability that the sample comes from a known class. Why do not directly use the output of $G_c$? The $K+1$ dimension of $G_c$  (i.e. $G_c^{K+1})$) seems like the same probability. }

\subsection*{\underline{\textbf{Response:}}}

For convenience, here we use $p_k$ to denote the probability that the sample comes from a known class . 
As a matter of fact, we do use the output of $G_c$ to measure $p_k$ by evaluating $K+1$ dimensional output (i.e. $G_c^{K+1}$). 
\textcolor{blue}{Eq.(3)} of the original manuscript indicates the measurement, which is as follows, 
\begin{align}
    w_d(x) &= 1-G_d(z). \label{eq: domain transferability}
\end{align}
Here $G_d$ denotes the last dimension of $G_c$ (\textit{i.e.,} $G_d = G_c^{K+1}$). 
The output of $G_d$ is an approximation of the probability that sample from the unknown class, since the model does not converge during training and lack of supervised information to identify the unknown class.
Therefore $w_d$ is an approximation of $p_s$.
In order to measure $p_s$ from a different perspective and make it more robust, we also utilize the entropy of probabilities of known classes as a measurement as indicated in \textcolor{blue}{Eq.(4)} and \textcolor{blue}{Eq.(5)}.
These equations are respectively as follows,
\begin{align}
    G_{c,\; known} &= softmax([G_c^1,G_c^2,...,G_c^K]).\\
    w_c(x) &= 1-H(G_{c,\; known}(z)). \label{eq: class transferability}
\end{align}
The intuition behind this approximation is that target samples from the known classes can be categorized by $G_{c, known}$.
As \textit{entropy} measures the degree to which the probability of the model is spread out over different possible states, it can be used to measure the uncertainty of predictions.
Due to the overlapping in the marginal distributions, the target samples from the known classes should be categorized by the classifier $G_c$ that trained on the source samples, leading to low entropy.
And for samples from the unknown class, because they cannot be aligned with a specific class from the source domain, the prediction tends to be uncertain over $K$ known categories, leading to high entropy.

In the revised manuscript, we revise \textit{Section 4.2 } to make a clearer presentation of the transferability criterion. 


\section{In \textcolor{blue}{Eq.(6)}, why is the probability of being a certain known class and the probability of being the unknown class independent? }
\subsection*{\underline{\textbf{Response:}}}

These probability can be considered independent are mainly contribute to two reasons:
\begin{itemize}[topsep=0pt]
    \small
    \item The probability of being a certain known class is independent from the probability of being a certain domain.
    \item The proposed model use the probability of being the target domain to approximate the probability of being the unknown class.
\end{itemize}
In the following, we give more detailed explanations.

In domain adaptation, the probability of being a certain known class is independent from the probability of being a certain domain.
That is because the classifier that leverages domain-invariant features gives the probability of being a certain known class.
The domain discriminator that leverages domain-relevant features gives the probability of from a certain domain.
Since the classifier and domain discriminator work independently, and domain-invariant and domain-relevant can be well separated from each other \cite{DomainAgnostic}, these probabilities can be considered as independent in domain adaptation tasks.

In open set domain adaptation, we can not directly give the probability of being the unknown class due to the lack of supervised information.
Instead, since samples from the unknown class are usually untransferable and are prone to be labeled as the target sample, we use the probability of being the target domain (\textit{i.e.,} the output of $G_c^{K+1}$) to approximate the probability of being the unknown class.
Thus, the probability of being a certain known class and the probability of being the unknown class can be considered independent in our work.


\section{In Fig.3, $G_fs$ for the source domain and target domain share weights.
However, samples from target domain should be transferred, so are there some differences between these two $G_f$s?
Or are there some extra structures after $G_f$ for target domain?}
\subsection*{\underline{\textbf{Response:}}}

Thanks for your precious comments.
In this work, only one $G_f$ is used for the source domain and target domain, and there is no extra structures after $G_f$ for target domain.
Such network designing is widely adopted in many works \cite{OpensetDA-bp,PartialDA-tf,UniversalDomainAdaptation}, and is demonstrated to be an effective way to transfer knowledge in domain adversarial training.

Specifically, the domain adversarial training is as follows,
\begin{equation}
    \label{eq: training DANN 1}
    \begin{split}
        \min_{G_f^t} \max_{G_d} \mathscr{L}(G_d,G^{s}_{f},G_f^t) &=\mathbb{E}_{x\sim p_t(x)} \left[ \log \left(G_d\left(G_f^t\left(x\right)\right)\right) \right]\\
        &+\mathbb{E}_{x\sim p_s(x)}\left[ \log \left(1-G_d\left(G_f^s\left(x\right)\right)\right) \right].
    \end{split}
\end{equation}
$G_f^s$ and $G_f^t$ are the feature extractors for source and target samples, which share weights as in \cite{OpensetDA-bp}.
In the training of Eq.(\ref{eq: training DANN 1}), $G_f^t$ is confined to generate features to confuse domain discriminator for target samples.
Therefore the distribution of the target domain can be aligned with the distribution of the source domain.
As the distribution is aligned, the classifier $G_c$ that trained with the source label information can perform well for target samples, so the knowledge learned on the source domain can be transferred to the target domain.


\section{The writing could be improved, especially for some confusing description.
Some typos and grammar errors should be corrected, such as "a certain known classes".}

\subsection*{\underline{\textbf{Response:}}}

Thanks for pointing out the mistake.
As suggested, the paper has been proofread and the language errors have been corrected in the revised manuscript.

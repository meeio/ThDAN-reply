\section{How does the domain discriminator $G_d$ work? $G_d$ should be explained when it first appears.}

\subsection*{\underline{\textbf{Response:}}}

Thanks for your comments and suggestions.
$G_d$ is a binary domain discriminator with all the source samples labelled as $0$ and all the target samples labelled as $1$.

$G_d$ is one of the critical components in domain adversarial training \cite{DomainAdversrialNetwork,ADDA,OpensetDA-bp}.
The domain adversarial training aligns distributions by performing a minmax game: a domain discriminator is trained to separate the feature representation of the source domain from the one of the target domain, at the same time, feature generators are trained to deceive the domain discriminator.
In this work, we adopt the training scheme proposed in \cite{OpensetDA-bp} to enable the domain discriminator to identify samples of unknown class for open set domain adaptation. 
Then we can reduce the domain shift by optimizing:
\begin{equation}
    \label{eq: training DANN}
    \begin{split}
        \min_{G_f^t} \max_{G_d} \mathscr{L}(G_d,G^{s}_{f},G_f^t) &=\mathbb{E}_{x\sim p_t(x)} \left[ \log \left(G_d\left(G_f^t\left(x\right)\right)\right) \right]\\
        &+\mathbb{E}_{x\sim p_s(x)}\left[ \log \left(1-G_d\left(G_f^s\left(x\right)\right)\right) \right].
    \end{split}
\end{equation}
Here $p_s (x)$ and $p_t (x)$ are probability distributions of source and target data respectively.
$G_f^s$ and $G_f^t$ are the feature extractors for source and target samples, which share weights as in \cite{OpensetDA-bp}.

To clarify how the domain discriminator $G_d$ and domain adversarial training works, we revised the manuscript by adding Section \textit{3.2: Domain Adversarial Training} for better understanding.


\section{In \textcolor{blue}{Eq.(2)}, $p_s$ and $p_t$ are probability distributions, so what do $p_s (z_t)$ and $p_t (z_t)$ mean? And why is the optimal $G_d$ this? }
\subsection*{\underline{\textbf{Response:}}}

Sorry for the misleading symbol notations.
In \textcolor{blue}{Eq.(2)} of the original manuscript, the subscript of $z_t$ should be removed and $z$ is the deep feature representation of a sample, \textit{i.e.,} $z=G_f(x).$
As stated in \textit{Section 3.1 open set domain adaptation} in the revised manuscript, $p_s (z)$ and $p_t (z)$ are probability distributions of feature representations in source and target domains respectively, as defined in \cite{PartialDA-iw}.
In the revised manuscript, \textcolor{blue}{Eq.(2)} is rewritten as:
\begin{equation}
    \label{eq: revised optimal}
    \begin{split}
        G_d^*(z) &= \frac{p_t(z)}{p_s(z)+p_t(z)}. \\
    \end{split}
\end{equation}

Similar to \cite{goodfellow2014generative}, the proof for the property that Eq.(\ref{eq: revised optimal}) gives the optimal $G_d$ with fixed $G_f^s$ and $G_f^t$ is shown as follows,
\begin{proof}
    For fixed $G_f^s$ and $G_f^t$, we train $G_d$ to maximize Eq.(\ref{eq: training DANN}):
    \begin{equation}
        \label{eq: proof optimal discriminator}
        \begin{split}
            \max_{G_d} \mathscr{L}(G_d,G^{s}_{f},G_f^t)  = &\int_x p_t(x)\log \left(G_d\left(G_f^t\left(x\right)\right)\right)
              + p_s(x) \log\left(1-G_d\left(G_f^s\left(x\right)\right)\right) \, dx.
            \\ = &\int_z p_t(z)\log \left(G_d\left(z\right)\right)
              + p_s(z) \log\left(1-G_d\left(z\right)\right) \, dz.
        \end{split}
    \end{equation}
    By taking the partial derivative of the objective Eq.(\ref{eq: proof optimal discriminator}) with respect to $G_d$ and exchanging the order of differentiation and integration with Leibnizs rule, the optimal $G_d$ is in $[0, 1]$ given by Eq.(\ref{eq: revised optimal}).
\end{proof}

The proof has been added in Section \textit{3.2: Domain Adversarial Training} in the revised manuscript.

\section{In \textcolor{blue}{Eq.(4)} and \textcolor{blue}{Eq.(5)}, authors use the entropy of probabilities of known classes to measure the probability that the sample comes from a known class. Why do not directly use the output of $G_c$? The $K+1$ dimension of $G_c$  (i.e. $G_c^{K+1})$) seems like the same probability. }

\subsection*{\underline{\textbf{Response:}}}

Thanks for your suggestion. 
The presentation in the original manuscript may not be clear enough. 
As a matter of fact, we have directly used the output of $G_c$ ($G_c^{K+1}$) to measure the probability that the sample comes from a known class. 
Such probability can be interpreted as the transferability for sample selection.

Due to lack of supervised signals to train a model to identify the unknown class, the output of $G_d$ is used to approximate the probability that the sample comes from the unknown class, i.e., $G_c^{K+1} = G_d$.
In the original manuscript, \textcolor{blue}{Eq.(3)} measures the transferability (or probability of known class) by $G_d$ as follows,
\begin{align}
    w_d(x) &= 1-G_d(z). \label{eq: domain transferability}
\end{align}

To increase robustness, the transferability (or probability of known class) is also computed from a different perspective. 
Thus, the entropy of probabilities of known classes is utilized as a measurement as in \textcolor{blue}{Eq.(4)} and \textcolor{blue}{Eq.(5)}, i.e.,
\begin{align}
    G_{c,\; known} &= softmax([G_c^1,G_c^2,...,G_c^K]).\\
    w_c(x) &= 1-H(G_{c,\; known}(z)). \label{eq: class transferability}
\end{align}
The intuition behind this approximation is that samples from the known classes can be categorized correctly by $G_{c, known}$.
As \textit{entropy} measures the degree to which the probability of the model is spread out over different possible states, it is used to measure the uncertainty of predictions.
Due to the overlapping in the marginal distributions across domains, target samples from the known classes could be categorized correctly by the classifier $G_{c, known}$ that trained on the source samples. 
This leads to low entropy (high transferability) for the samples from the known classes.
For the samples from the unknown class, because they cannot be classified into one of the $K$ known classes, the uncertainty of the prediction measured by the entropy is large.

In the revised manuscript, we have modified Section \textit{4.2} to give a clearer presentation of the transferability criterion.


\section{In \textcolor{blue}{Eq.(6)}, why is the probability of being a certain known class and the probability of being the unknown class independent? }
\subsection*{\underline{\textbf{Response:}}}

Thanks for your comments.
We notice that the phrase ``the probability of being a certain known class and the probability of being the unknown class independent'' is not precise.
This phrase is revised as ``Eq.(\ref{eq: domain transferability}) and Eq.(\ref{eq: class transferability}) measures the transferability from independent perspectives'' for better explaining \textcolor{blue}{Eq.(6)}.

To measure the transferability, Eq.(\ref{eq: domain transferability}) and Eq.(\ref{eq: class transferability}) respectively evaluate the probability of being target domain and the entropy of probabilities of known classes.
In the context of domain adaptation, the probability of being target domain and the probabilities of known classes are independent.
This is because the domain discriminator $G_d$ and the classifier $G_{c,\; known}$ are implemented independently, and the domain-relevant and domain-invariant features leveraged by them can be well separated from each other by domain adversarial training \cite{DomainAgnostic}.
Therefore Eq.(\ref{eq: domain transferability}) and Eq.(\ref{eq: class transferability}) are two independent perspectives of measuring the transferability.

In the revised manuscript, we have modified Section \textit{4.2} to give a more precise explanation for \textcolor{blue}{Eq.(6)}.



\section{In Fig.3, $G_fs$ for the source domain and target domain share weights.
However, samples from target domain should be transferred, so are there some differences between these two $G_f$s?
Or are there some extra structures after $G_f$ for target domain?}
\subsection*{\underline{\textbf{Response:}}}

Thanks for your precious comments.
In this work, $G_f^s$ and $G_f^t$ denote the feature extractors for source and target samples, respectively. 
They share weights as in \cite{OpensetDA-bp}, so there is no difference between them. 
Also, there is no extra structure after $G_f^t$ for the target domain.
Such network design has been widely adopted in existing domain adaptation works \cite{OpensetDA-bp,PartialDA-tf,UniversalDomainAdaptation}.

To transfer knowledge between domains, the domain adversarial training is formulated as follows,
\begin{equation}
    \label{eq: training DANN 1}
    \begin{split}
        \min_{G_f^t} \max_{G_d} \mathscr{L}(G_d,G^{s}_{f},G_f^t) &=\mathbb{E}_{x\sim p_t(x)} \left[ \log \left(G_d\left(G_f^t\left(x\right)\right)\right) \right]\\
        &+\mathbb{E}_{x\sim p_s(x)}\left[ \log \left(1-G_d\left(G_f^s\left(x\right)\right)\right) \right].
    \end{split}
\end{equation}
% $G_f^s$ and $G_f^t$ are the feature extractors for source and target samples, which share weights as in \cite{OpensetDA-bp}.
By optimizing Eq.(\ref{eq: training DANN 1}) with fixed $G_d$, $G_f^t$ is confined to generate features to confuse the domain discriminator for target samples.
Therefore the distribution of the target domain can be aligned with the distribution of the source domain.
As the distributions are aligned, the classifier $G_c$ trained with the source label information can perform well for target samples, so the knowledge learned on the source domain can be transferred to the target domain.


\section{The writing could be improved, especially for some confusing description.
Some typos and grammar errors should be corrected, such as "a certain known classes".}

\subsection*{\underline{\textbf{Response:}}}

Thanks for your precious comments.
As suggested, the paper has been proofread and the typos and grammar errors have been corrected in the revised manuscript.
